{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRQN 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMpklEQVR4nO3df+xd9V3H8efLFsZgklIopFJiISEMYkLBbyaIMQpDERfwj81AFrMYkv0zFdySrehfS/yDJWZjf5glDWwSg/wYA0eahdl0LMbEdLSAG1AYhSF8hdE6QeaWqN3e/nFO9Zv6rT3f7/3e7/eefp6P5Obec+69OZ+T01fPj++573eqCkknvp9Z6wFIWh2GXWqEYZcaYdilRhh2qRGGXWrERGFPcl2SF5IcSLJ9pQYlaeVluX9nT7IO+C5wLTAPPAHcXFXPrdzwJK2U9RN8933Agap6GSDJ/cCNwDHDftZZZ9XWrVsnWOSJb9++fWs9BI1cVWWx+ZOE/VzgtQXT88Av/X9f2Lp1K3v37p1gkSe+ZNHtJE1sknP2xf5V/p9zgiQfTbI3yd5Dhw5NsDhJk5gk7PPAeQumtwCvH/2hqtpRVXNVNbdp06YJFidpEpOE/QngwiTnJzkZuAl4dGWGJWmlLfucvaoOJ/kD4OvAOuCLVfXsio1M0oqa5AIdVfU14GsrNBZJU+QddFIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjjhv2JF9McjDJMwvmbUyyK8mL/fMZ0x2mpEkN2bP/JXDdUfO2A7ur6kJgdz8taYYdN+xV9XfAvx41+0bgnv71PcDvrPC4JK2w5Z6zn1NVbwD0z2ev3JAkTcPUL9DZEUaaDcsN+5tJNgP0zweP9UE7wkizYblhfxT4SP/6I8BXV2Y4kqZlyJ/e7gP+AbgoyXySW4A7gGuTvEjXn/2O6Q5T0qSO2xGmqm4+xlvXrPBYJE2Rd9BJjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjRhSluq8JI8n2Z/k2SS39vPtCiONyJA9+2HgE1V1MXAF8LEkl2BXGGlUhnSEeaOqnuxf/xDYD5yLXWGkUVnSOXuSrcBlwB4GdoWxSYQ0GwaHPcl7gK8At1XVO0O/Z5MIaTYMCnuSk+iCfm9VPdzPHtwVRtLaG3I1PsDdwP6q+uyCt+wKI43IcZtEAFcBvwd8J8nT/bw/oesC82DfIeZV4EPTGaKklTCkI8zfAznG23aFkUbCO+ikRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRFDfs+uVVVrPYAZd6xfW+t43LNLjTDsUiOG1KA7Jcm3kvxj3xHm0/3885Ps6TvCPJDk5OkPV9JyDdmz/wdwdVVdCmwDrktyBfAZ4HN9R5i3gFumN0xJkxrSEaaq6t/7yZP6RwFXAw/18+0II824oXXj1/WVZQ8Cu4CXgLer6nD/kXm6llCLfdeOMNIMGBT2qvpJVW0DtgDvAy5e7GPH+K4dYaQZsKSr8VX1NvBNum6uG5Ic+Tv9FuD1lR2apJU05Gr8piQb+tfvBt5P18n1ceCD/cfsCCPNuCF30G0G7kmyju4/hwerameS54D7k/wZ8BRdiyhJM2pIR5hv07VpPnr+y3Tn75JGwDvopEYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYMDntfTvqpJDv7aTvCSCOylD37rXSFJo+wI4w0IkObRGwBfhu4q58OdoSRRmXonv1O4JPAT/vpM7EjjDQqQ+rGfwA4WFX7Fs5e5KN2hJFm2JC68VcBNyS5HjgFOJ1uT78hyfp+725HGGnGDenientVbamqrcBNwDeq6sPYEUYalUn+zv4p4ONJDtCdw9sRRpphQw7j/0dVfZOusaMdYaSR8Q46qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGDKtUkeQX4IfAT4HBVzSXZCDwAbAVeAX63qt6azjAlTWope/Zfr6ptVTXXT28HdvcdYXb305Jm1CSH8TfSdYIBO8JIM29o2Av42yT7kny0n3dOVb0B0D+fvdgX7QgjzYah1WWvqqrXk5wN7Ery/NAFVNUOYAfA3Nzcol1jJE3foD17Vb3ePx8EHqErIf1mks0A/fPBaQ1S0uSG9Ho7LcnPHnkN/AbwDPAoXScYsCOMNPOGHMafAzzSdWlmPfDXVfVYkieAB5PcArwKfGh6w5Q0qeOGve/8cuki838AXDONQUlaed5BJzXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71IihP4TRqslaD0AnKPfsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjRgU9iQbkjyU5Pkk+5NcmWRjkl1JXuyfz5j2YCUt39A9++eBx6rqvXQlqvZjRxhpVIZUlz0d+FXgboCq+s+qehs7wkijMmTPfgFwCPhSkqeS3NWXlLYjjDQiQ8K+Hrgc+EJVXQb8iCUcslfVjqqaq6q5TZs2LXOYkiY1JOzzwHxV7emnH6ILvx1hpBE5btir6vvAa0ku6mddAzyHHWGkURn6e/Y/BO5NcjLwMvD7dP9R2BFGGolBYa+qp4G5Rd6yI4w0Et5BJzXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71IghpaQvSvL0gsc7SW6zSYQ0LkNq0L1QVduqahvwi8CPgUewSYQ0Kks9jL8GeKmq/gmbREijstSw3wTc178e1CRC0mwYHPa+suwNwJeXsgA7wkizYSl79t8CnqyqN/vpQU0i7AgjzYalhP1m/vcQHmwSIY3K0P7spwLXAg8vmH0HcG2SF/v37lj54UlaKUObRPwYOPOoeT/AJhHSaHgHndQIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9SIoWWp/jjJs0meSXJfklOSnJ9kT98R5oG++qykGTWk/dO5wB8Bc1X1C8A6uvrxnwE+13eEeQu4ZZoDlTSZoYfx64F3J1kPnAq8AVwNPNS/b0cYacYN6fX2z8CfA6/ShfzfgH3A21V1uP/YPHDutAYpaXJDDuPPoOvrdj7wc8BpdA0jjlbH+L4dYaQZMOQw/v3A96rqUFX9F13t+F8GNvSH9QBbgNcX+7IdYaTZMCTsrwJXJDk1SehqxT8HPA58sP+MHWGkGTfknH0P3YW4J4Hv9N/ZAXwK+HiSA3QNJO6e4jglTShVi55qT8Xc3Fzt3bt31ZY3Rt3Bk7R8VbXoPyLvoJMaYdilRhh2qRGGXWrEql6gS3II+BHwL6u20Ok7C9dnVp1I6wLD1ufnq2rRG1pWNewASfZW1dyqLnSKXJ/ZdSKtC0y+Ph7GS40w7FIj1iLsO9ZgmdPk+syuE2ldYML1WfVzdklrw8N4qRGrGvYk1yV5IcmBJNtXc9mTSnJekseT7O/r8d3az9+YZFdfi29X//v/0UiyLslTSXb206OtLZhkQ5KHkjzfb6crx7x9Vrr246qFPck64C/oCl9cAtyc5JLVWv4KOAx8oqouBq4APtaPfzuwu6/Ft7ufHpNbgf0LpsdcW/DzwGNV9V7gUrr1GuX2mUrtx6palQdwJfD1BdO3A7ev1vKnsD5fBa4FXgA29/M2Ay+s9diWsA5b6AJwNbATCN1NG+sX22az/ABOB75Hfx1qwfxRbh+6Mm+vARvpakDuBH5zku2zmofxRwZ/xGjr1iXZClwG7AHOqao3APrns9duZEt2J/BJ4Kf99JmMt7bgBcAh4Ev9acldSU5jpNunplD7cTXDvthvbEf3p4Ak7wG+AtxWVe+s9XiWK8kHgINVtW/h7EU+OpZttB64HPhCVV1Gd1v2KA7ZFzNp7cfFrGbY54HzFkwfs27drEpyEl3Q762qh/vZbybZ3L+/GTi4VuNboquAG5K8AtxPdyh/JwNrC86geWC+uspK0FVXupzxbp+Jaj8uZjXD/gRwYX818WS6iw2PruLyJ9LX37sb2F9Vn13w1qN0NfhgRLX4qur2qtpSVVvptsU3qurDjLS2YFV9H3gtyUX9rCO1Eke5fZhG7cdVvuhwPfBd4CXgT9f6IsgSx/4rdIdM3wae7h/X053n7gZe7J83rvVYl7Fuvwbs7F9fAHwLOAB8GXjXWo9vCeuxDdjbb6O/Ac4Y8/YBPg08DzwD/BXwrkm2j3fQSY3wDjqpEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVG/DfHl8lCKssbmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=True, size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,rnn_cell,myScope):\n",
    "        # 네트워크는 게임으로부터 하나의 프레임을 받아 이를 배열로 만든다(flattening).\n",
    "        # 그 다음 배열의 크기를 재조절하고 4개의 합성곱 계층을 거처 처리한다.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,\\\n",
    "            kernel_size=[8,8],stride=[4,4],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv1')\n",
    "        self.conv2 = slim.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,\\\n",
    "            kernel_size=[4,4],stride=[2,2],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv2')\n",
    "        self.conv3 = slim.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,\\\n",
    "            kernel_size=[3,3],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv3')\n",
    "        self.conv4 = slim.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,\\\n",
    "            kernel_size=[7,7],stride=[1,1],padding='VALID', \\\n",
    "            biases_initializer=None,scope=myScope+'_conv4')\n",
    "        \n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        # 마지막 합성곱 계층에서 출력값을 취한 후 이를 순환 계층에 보낸다.\n",
    "        # 입력값은 RNN 처리를 위해 [batch x trace x units]으로 크기를 바꿔야 하며\n",
    "        # 상위 레벨로 전달될 때 [batch x units] 형태로 리턴해야 한다.\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.conv4),[self.batch_size,self.trainLength,h_size])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.convFlat,cell=rnn_cell,dtype=tf.float32,initial_state=self.state_in,scope=myScope+'_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "        # 순환 플레이어의 출력값을 어드밴티지 스트림과 가치 스트림으로 분리한다.\n",
    "        self.streamA,self.streamV = tf.split(self.rnn,2,1)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,4]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "        # 최종 Q 값을 얻기 위해 어드밴티지 스트림과 가치 스트림을 조합\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        # 타깃 Q 값과 예측 Q 값의 차의 제곱합을 구함으로써 비용을 얻는다.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,4,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        # 정확한 경사만 네트워크에 전파하기 위해\n",
    "        # 각각의 추이에 대해 비용의 앞쪽 1/2을 마스크 처리한다.\n",
    "        self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "        self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "        self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "        self.mask = tf.reshape(self.mask,[-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size]=[]\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size, trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer, batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranining Parameters\n",
    "batch_size = 4 \n",
    "trace_length = 8\n",
    "update_freq = 5\n",
    "y = .99\n",
    "startE = 1\n",
    "endE = 0.1\n",
    "anneling_steps = 10000\n",
    "num_episodes = 10000\n",
    "pre_train_steps = 10000\n",
    "load_model = False\n",
    "path = './drqn'\n",
    "h_size = 512\n",
    "max_epLength = 50\n",
    "time_per_step = 1\n",
    "summaryLength = 100\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Set Success\n",
      "5000 0.69 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'main'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-84eb20528505>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msummaryLength\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             saveToCenter(i,rList,jList,np.reshape(np.array(episodeBuffer),[len(episodeBuffer),5]),\\\n\u001b[1;32m--> 117\u001b[1;33m                 summaryLength,h_size,sess,mainQN,time_per_step)\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/model-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.cptk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\ReinforcementLearning\\Hyungjun\\helper.py\u001b[0m in \u001b[0;36msaveToCenter\u001b[1;34m(i, rList, jList, bufferArray, summaryLength, h_size, sess, mainQN, time_per_step)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msalience\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                                        \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbufferArray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                                                                                \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m21168\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.0\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainLength\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate_display\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mimagesS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mimagesS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimagesS\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagesS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagesS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagesS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mimagesS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagesS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mimagesS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagesS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagesS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'main'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#We define the cells for the primary and target q-networks\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main')\n",
    "targetQN = Qnetwork(h_size,cellT,'target')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "##Write the first line of the master log-file for the Control Center\n",
    "with open('./Center/log.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \n",
    "  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "   \n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "        #Reset environment and get first new observation\n",
    "        sP = env.reset()\n",
    "        s = processState(sP)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size])) #Reset the recurrent layer's hidden state\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            s1P,r,d = env.step(a)\n",
    "            s1 = processState(s1P)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    updateTarget(targetOps,sess)\n",
    "                    #Reset the recurrent layer's hidden state\n",
    "                    state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size])) \n",
    "                    \n",
    "                    trainBatch = myBuffer.sample(batch_size,trace_length) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={\\\n",
    "                        mainQN.scalarInput:np.vstack(trainBatch[:,3]/255.0),\\\n",
    "                        mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={\\\n",
    "                        targetQN.scalarInput:np.vstack(trainBatch[:,3]/255.0),\\\n",
    "                        targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:batch_size})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size*trace_length),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]/255.0),mainQN.targetQ:targetQ,\\\n",
    "                        mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n",
    "                        mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        #Add the episode to the experience buffer\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        episodeBuffer = list(zip(bufferArray))\n",
    "        myBuffer.add(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print (\"Saved Model\")\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print (total_steps,np.mean(rList[-summaryLength:]), e)\n",
    "            saveToCenter(i,rList,jList,np.reshape(np.array(episodeBuffer),[len(episodeBuffer),5]),\\\n",
    "                summaryLength,h_size,sess,mainQN,time_per_step)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0.01 #The chance of chosing a random action\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "load_model = True #Whether to load a saved model.\n",
    "path = \"./drqn\" #The path to save/load our model to/from.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "time_per_step = 1 #Length of each step used in gif creation\n",
    "summaryLength = 100 #Number of epidoes to periodically save for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main')\n",
    "targetQN = Qnetwork(h_size,cellT,'target')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "##Write the first line of the master log-file for the Control Center\n",
    "with open('./Center/log.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['Episode','Length','Reward','IMG','LOG','SAL'])    \n",
    "    \n",
    "    #wr = csv.writer(open('./Center/log.csv', 'a'), quoting=csv.QUOTE_ALL)\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "\n",
    "        \n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "        #Reset environment and get first new observation\n",
    "        sP = env.reset()\n",
    "        s = processState(sP)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size]))\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e:\n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s/255.0],mainQN.trainLength:1,\\\n",
    "                    mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            s1P,r,d = env.step(a)\n",
    "            s1 = processState(s1P)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "\n",
    "        #Periodically save the model. \n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print (total_steps,np.mean(rList[-summaryLength:]), e)\n",
    "            saveToCenter(i,rList,jList,np.reshape(np.array(episodeBuffer),[len(episodeBuffer),5]),\\\n",
    "                summaryLength,h_size,sess,mainQN,time_per_step)\n",
    "print (\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
