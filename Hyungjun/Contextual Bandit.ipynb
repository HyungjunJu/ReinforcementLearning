{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Bandit Problem - 콘텍스트 밴딧 문제\n",
    "\n",
    "가장 simple한 형태인 MAB 문제에서, State의 개념을 도입한 문제\n",
    "1개의 밴딧이 아니라 여러 개의 밴딧에 대해 고려\n",
    "각각의 밴딧에서 최선의 action을 선택하여 잠재적 보상을 최선으로 하는 것이 목표\n",
    "\n",
    "MAB에서 구현한 네트워크를 확장하여 신경망을 이용, 상태를 입력으로 받아 액션을 출력하는 신경망\n",
    "학습에는 Policy Gradient(정책 경사) Update 활용. 이를 통해 State와 Action의 매핑을 학습.\n",
    "결국 네트워크에는 특정 상태의 맥락에서의 해당 action에 대한 값을 네트워크 가중치로 가지게 됨.\n",
    "또한, 정책 Pi의 의미도 네트워크 가중치라는 맥락에서 출력 계층에서 선택된 Action을 의미하게 됨.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        # 밴딧들의 손잡이 목록을 작성. 각 밴딧은 각각 손잡이 4, 2, 1이 최적임\n",
    "        self.bandits = np.array([[0.2,0,-0.1,-5],[0.1,-5,1.0,0.25],[-5,5,5,5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        # 각각의 에피소드에 대해 랜덤한 상태를 반환\n",
    "        self.state = np.random.randint(0,len(self.bandits))\n",
    "        return self.state\n",
    "    \n",
    "    def pullArm(self,action):\n",
    "        # 랜덤한 수를 얻는다.\n",
    "        bandit = self.bandits[self.state,action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            # 양의 보상을 반환한다.\n",
    "            return 1\n",
    "        else:\n",
    "            # 음의 보상을 반환한다.\n",
    "            return -1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        # 네트워크의 피드포워드 부분, 에이전트는 상태를 받아서 액션을 출력한다.\n",
    "        self.state_in= tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "        state_in_OH= slim.one_hot_encoding(self.state_in,s_size)\n",
    "        output= slim.fully_connected(state_in_OH, a_size,\\\n",
    "                                     biases_initializer=None, activation_fn=tf.nn.sigmoid,\\\n",
    "                                     weights_initializer=tf.ones_initializer())\n",
    "        self.output = tf.reshape(output,[-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        # 학습 과정을 구현한다.\n",
    "        # 비용을 계산하기 위해 보상과 선택된 액션을 네트워크에 피드하고, \n",
    "        # 네트워크를 업데이트하는 데에 이를 이용한다.\n",
    "        self.reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
    "        self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_reward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6c7018ba6cd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean reward for each of the \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcBandit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_bandits\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" bandits: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_reward' is not defined"
     ]
    }
   ],
   "source": [
    "# 텐서플로 그래프를 리셋한다\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 밴딧을 로드한다.\n",
    "cBandit = contextual_bandit()\n",
    "# 에이전트를 로드한다.\n",
    "myAgent = agent(lr=0.001, s_size=cBandit.num_bandits,a_size=cBandit.num_actions)\n",
    "# 네트워크 내부를 들여다보기 위해 평가할 가중치\n",
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "# 에이전트를 학습시킬 전체 에피소드 수 설정\n",
    "total_episodes = 10000\n",
    "# 밴딧에 대한 점수판을 0으로 설정\n",
    "total_reward = np.zeros([cBandit.num_bandits,cBandit.num_actions])\n",
    "# 랜덤한 액션을 취할 가능성\n",
    "e = 0.1\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 텐서플로 그래프 론칭\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        # 환경으로부터 상태 가져오기\n",
    "        s = cBandit.getBandit()\n",
    "        # 네트워크로부터 랜덤한 액션 또는 하나의 액션을 선택한다.\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action, feed_dict={myAgent.state_in:[s]})\n",
    "        \n",
    "        # 주어진 밴딧에 대해 액션을 취한 데 대한 보상을 얻는다.\n",
    "        reward = cBandit.pullArm(action)\n",
    "        \n",
    "        # 네트워크를 업데이트한다.\n",
    "        feed_dict={myAgent.reward_holder:[reward],\\\n",
    "                  myAgent.action_holder:[action],myAgent.state_in:[s]}\n",
    "        _,ww = sess.run([myAgent.update,weights], feed_dict=feed_dict)\n",
    "        \n",
    "        # 보상의 총계 업데이트\n",
    "        total_reward[s,action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print(\"Mean reward for each of the \" + str(cBandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward, axis=1)))\n",
    "        i+=1\n",
    "   \n",
    "\n",
    "for a in range(cBandit.num_bandits):\n",
    "    print(\"The agent thinks action \" + str(np.argmax(ww[a]+1)) + \" for bandit \" + str(a+1) + \" is the most promising...\")\n",
    "    if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n",
    "        print(\"...and it was right!\")\n",
    "    else:\n",
    "        print(\"...and it was worng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
