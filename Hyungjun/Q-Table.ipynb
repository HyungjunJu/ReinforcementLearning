{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning을 구현 - Table Driven\n",
    "이 파일에서는 gym-FrozenLake 환경에서 Q table 을 학습 및 채워 나가는 코드를 구현\n",
    "밸만 방정식: 장기적 보상의 최대화는 현재 선택한 액션에서의 즉각적 보상과 최선의 미래에서 주어지는 보상의 합과 같다는 것이 맥락"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 테이블을 모두 0으로 초기화\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# 학습 매개변수를 결정 (HyperParameter)\n",
    "lr = .85\n",
    "y = .99 #discount factor\n",
    "num_episodes = 2000\n",
    "# 보상의 총계를 담을 리스트를 생성한다.\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    # 환경을 리셋하고 첫 번째 새로운 관찰을 수행한다.\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    # Q 테이블 학습 알고리즘\n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        # Q 테이블로부터 (노이즈와 함께 - 바람?) 그리디하게 액션을 선택\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        # 환경으로부터 새로운 상태와 보상을 얻는다.\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        # 새로운 지식을 통해 Q 테이블을 업데이트 한다.\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time : 0.4145\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time : \" + str(sum(rList)/num_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table values\n",
      "[[5.97548985e-03 5.57415422e-01 2.01412620e-02 2.00400315e-02]\n",
      " [0.00000000e+00 2.09454054e-03 4.77767236e-04 2.71790293e-01]\n",
      " [1.98532891e-02 4.11424688e-03 1.93325469e-03 2.03770272e-01]\n",
      " [1.32743005e-02 1.15876833e-02 0.00000000e+00 1.99391979e-01]\n",
      " [6.53391722e-01 0.00000000e+00 1.34662327e-04 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.97531901e-01 3.16863907e-04 6.98818798e-05 1.30321641e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 4.35211128e-03 1.88613301e-02 3.60163707e-01]\n",
      " [0.00000000e+00 4.59079796e-01 4.04271573e-03 5.58173226e-04]\n",
      " [7.56830947e-01 4.31947755e-04 6.01094796e-04 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 6.97752021e-05 8.93093988e-01 7.21023039e-05]\n",
      " [0.00000000e+00 0.00000000e+00 9.72313908e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Table values\")\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
